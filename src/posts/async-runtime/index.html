<div>
    <section id="intro">
        <hgroup class="center py1">
            <h2 class="mb1">Writing an Async Runtime (Zig)</h2>
            <span>Oct. 16th, 2024</span>
        </hgroup>
        <p>
        While working on zzz, I thought about how I would trigger asynchronous events when already operating in an asynchronous context. An easy way to think about this is making a database call while inside of an HTTP request handler. Our response depends on this database call but we don't want to block execution on our thread while this query resolves.
        </p>
        <p>
        As I wrote about <a href="/post/modular-async">earlier</a>, Zig doesn't have any async primitives in the language. As a result, this style of execution was not supported by zzz's event loop style and requres some higher level management of execution.
        </p>
        <p>
        A language with <code class="language-rust">async/await</code> will automatically handle this case for you in a pretty interesting way.<sup>[1][2]</sup>. This allows your asynchronous code to look like synchronous code. The example below will fetch a set of items from a database and then generate an HTML fragment using a template that depends on this list of items. Behavior like this is currently impossible in zzz.
        </p>
        <small>[1]: <a href="https://www.eventhelix.com/rust/rust-to-assembly-async-await/">Understanding Async Await in Rust: From State Machines to Assembly Code</a></small> 
        <br>
        <small>[2]: <a href="https://cliffle.com/blog/async-inversion/#async-fn-is-an-inversion-of-control">How to think about async/await in Rust</a></small>
        <br>
        <pre style="max-width: 80vw">
            <code class="language-rust">
            pub async fn get_store_items(
                Extension(db): Extension<PgPool>,
                query: Option<Query<Pagination>>,
            ) -> impl IntoResponse {
                let Query(query) = query.unwrap_or_default();

                let Ok(items) = sqlx::query_as!(
                    ItemOnDisplayInner,
                    r#"
                    SELECT id, name, price, thumbnail_url
                    FROM items
                    ORDER BY entry_date DESC
                    OFFSET $1 ROWS
                    FETCH NEXT $2 ROWS ONLY
                    "#,
                    (query.page * query.per_page) as i64,
                    query.per_page as i64,
                )
                .fetch_all(&db)
                .await
                else {
                    return (StatusCode::NOT_FOUND, Html("".to_string()));
                };

                debug!("Store Items: {items:?}");
                let template = ItemsTemplate {
                    items: items.into_iter().map(|item| item.into()).collect(),
                };

                (StatusCode::OK, Html(template.render().unwrap()))
            }
            </code>
        </pre>
    </section>
    <section id="async-io">
        <h3>Asynchronous I/O</h3>
        <p>
        The nice thing about having built zzz first is that I already had a good foundation of asynchronous I/O to work off of. This asynchronous I/O provides the functionality for interacting with the filesystem and the network, by providing various queue operations.
        </p>
        <p>
        Currently, there are three supported asynchronous backends, <code class="language-zig">io_uring</code>, <code class="language-zig">epoll</code>, and <code class="language-zig">busy_loop</code>. The first two are for supporting Linux systems while the <code class="language-zig">busy_loop</code> implementation supports Linux, Mac and Windows. There is also support for custom asynchronous I/O backends that can be passed in at compile time.
        </p>
        <pre style="max-width: 80vw">
            <code class="language-zig">
            pub fn queue_open(
                self: *AsyncIO,
                task: usize,
                path: []const u8,
            ) !void {
                const uring: *AsyncIoUring = @ptrCast(@alignCast(self.runner));
                const borrowed = try uring.jobs.borrow_hint(task);
                borrowed.item.* = .{
                    .index = borrowed.index,
                    .type = .{ .open = path },
                    .task = task,
                    .fd = undefined,
                };

                _ = try uring.inner.openat(
                    @intFromPtr(borrowed.item),
                    std.posix.AT.FDCWD,
                    @ptrCast(path.ptr),
                    .{},
                    0,
                );
            }
            </code>
        </pre>
        <p>
        Above is an example of queuing a file open with the Asynchronous backend. This code operates within the <code class="language-zig">io_uring</code> backend. We utilize a pool of Job items that allow us to track which action completed later on when we reap events. We borrow (using a hint which means we just provide a starting index) from the pool and set a variety of parameters.
        </p>
        <p>
        There are a variety of other methods that operate in a similar way that have been omitted for brevity but they all handle various operations you want with files or the network.
        </p>
        <p>
        The important part of this asynchronous I/O system is that it allows us to queue various events and then handle them later when they complete. This callback approach is instrumental in building our runtime.
        </p>
    </section>
    <section id="scheduler">
        <h3>Adding a Scheduler</h3>
        <p>
        Now that we have a way to queue asynchronous I/O events and defer handling the result, we will build a scheduler. This will be a fairly simple scheduler that will run tasks in order to completion. When all of the tasks have been run, we will have an opporunity to reap and handle all of the asynchronous I/O events that have completed so far and then we repeat. 
        </p>
        <pre style="max-width: 80vw">
            <code class="language-zig">
                pub fn run(self: *Runtime) !void {
                    while (true) {
                        var iter = self.scheduler.runnable.iterator(.{ .kind = .set });
                        while (iter.next()) |index| {
                            const task: *Task = &self.scheduler.tasks.items[index];
                            assert(task.state == .runnable);

                            const cloned_task: Task = task.*;
                            task.state = .dead;
                            try self.scheduler.release(task.index);

                            @call(.auto, cloned_task.func, .{
                                self,
                                &cloned_task,
                                cloned_task.context,
                            }) catch |e| {
                                log.debug("task failed: {}", .{e});
                            };
                        }

                        try self.aio.submit();

                        const completions = try self.aio.reap();
                        for (completions) |completion| {
                            const index = completion.task;
                            const task = &self.scheduler.tasks.items[index];
                            assert(task.state == .waiting);
                            task.result = completion.result;
                            self.scheduler.set_runnable(index);
                        }

                        if (self.scheduler.runnable.count() == 0) {
                            log.err("no more runnable tasks", .{});
                            break;
                        }
                    }
                }
            </code>
        </pre>
        <p>
        A nicety about this scheduler model is that it provides a "green thread" implementation for free. You can manually spawn tasks into the runtime that don't depend on I/O to run concurrently. We can do this by adding the ability for the runtime to signal to the asynchronous I/O that it wishes to wait for a completion. If we end up having any runnable tasks(meaning they are not waiting on an I/O completion), we can force the asynchronous I/O to not wait.
        </p>
        <pre style="max-width: 80vw">
            <code class="language-zig">
            const wait_for_io = self.scheduler.runnable.count() == 0;
            log.debug("Wait for I/O: {}", .{wait_for_io});

            const completions = try self.aio.reap(wait_for_io);
            for (completions) |completion| {
                const index = completion.task;
                const task = &self.scheduler.tasks.items[index];
                assert(task.state == .waiting);
                task.result = completion.result;
                self.scheduler.set_runnable(index);
            }

            if (self.scheduler.runnable.count() == 0) {
                log.err("no more runnable tasks", .{});
                break;
            }
            </code>
        </pre>
    </section>
    <section id="example">
        <h3>TCP Echo Example</h3>
        <p>
        An easy way to do a proof of concept is to write a program that uses TCP to echo data. Below will be an example program that does:
        <ol>
            <li>Create a socket</li>
            <li>Accept on the socket</li>
            <li>Set the accepted socket to nonblocking</li>
            <li>Recv on that socket</li>
            <li>Send back what was recieved</li>
        </ol>
        </p>
        <pre style="max-width: 80vw">
            <code class="language-zig">
            const std = @import("std");
            const Pool = @import("tardy").Pool;
            const Runtime = @import("tardy").Runtime;
            const Task = @import("tardy").Task;
            const Tardy = @import("tardy").Tardy(.auto);

            const Provision = struct {
                index: usize,
                socket: std.posix.socket_t,
                buffer: []u8,
            };

            fn socket_to_nonblocking(socket: std.posix.socket_t) !void {
                const current_flags = try std.posix.fcntl(socket, std.posix.F.GETFL, 0);
                var new_flags = @as(
                    std.posix.O,
                    @bitCast(@as(u32, @intCast(current_flags))),
                );
                new_flags.NONBLOCK = true;
                const arg: u32 = @bitCast(new_flags);
                _ = try std.posix.fcntl(socket, std.posix.F.SETFL, arg);
            }

            fn close_connection(provision_pool: *Pool(Provision), provision: *const Provision) void {
                std.posix.close(provision.socket);
                provision_pool.release(provision.index);
            }

            fn accept_task(rt: *Runtime, t: *const Task, ctx: ?*anyopaque) !void {
                const server_socket: *std.posix.socket_t = @ptrCast(@alignCast(ctx.?));
                const child_socket = t.result.?.socket;
                try socket_to_nonblocking(child_socket);

                try rt.net.accept(.{
                    .socket = server_socket.*,
                    .func = accept_task,
                    .ctx = ctx,
                });

                const provision_pool: *Pool(Provision) = @ptrCast(@alignCast(rt.storage.get("provision_pool").?));
                const borrowed = try provision_pool.borrow();
                borrowed.item.index = borrowed.index;
                borrowed.item.socket = child_socket;
                try rt.net.recv(.{
                    .socket = child_socket,
                    .buffer = borrowed.item.buffer,
                    .func = recv_task,
                    .ctx = borrowed.item,
                });
            }

            fn recv_task(rt: *Runtime, t: *const Task, ctx: ?*anyopaque) !void {
                const provision: *Provision = @ptrCast(@alignCast(ctx.?));
                const length = t.result.?.value;

                if (length <= 0) {
                    const provision_pool: *Pool(Provision) = @ptrCast(@alignCast(rt.storage.get("provision_pool").?));
                    close_connection(provision_pool, provision);
                    return;
                }

                try rt.net.send(.{
                    .socket = provision.socket,
                    .buffer = provision.buffer[0..@intCast(length)],
                    .func = send_task,
                    .ctx = ctx,
                });
            }

            fn send_task(rt: *Runtime, t: *const Task, ctx: ?*anyopaque) !void {
                const provision: *Provision = @ptrCast(@alignCast(ctx.?));
                const length = t.result.?.value;

                if (length <= 0) {
                    const provision_pool: *Pool(Provision) = @ptrCast(@alignCast(rt.storage.get("provision_pool").?));
                    close_connection(provision_pool, provision);
                    return;
                }

                try rt.net.recv(.{
                    .socket = provision.socket,
                    .buffer = provision.buffer,
                    .func = recv_task,
                    .ctx = ctx,
                });
            }

            pub fn main() !void {
                const allocator = std.heap.page_allocator;
                const size = 1024;

                var tardy = try Tardy.init(.{
                    .allocator = allocator,
                    .threading = .single,
                });
                defer tardy.deinit();

                const host = "0.0.0.0";
                const port = 9862;

                const addr = try std.net.Address.resolveIp(host, port);

                var socket: std.posix.socket_t = blk: {
                    const socket_flags = std.posix.SOCK.STREAM | std.posix.SOCK.CLOEXEC | std.posix.SOCK.NONBLOCK;
                    break :blk try std.posix.socket(
                        addr.any.family,
                        socket_flags,
                        std.posix.IPPROTO.TCP,
                    );
                };

                if (@hasDecl(std.posix.SO, "REUSEPORT_LB")) {
                    try std.posix.setsockopt(
                        socket,
                        std.posix.SOL.SOCKET,
                        std.posix.SO.REUSEPORT_LB,
                        &std.mem.toBytes(@as(c_int, 1)),
                    );
                } else if (@hasDecl(std.posix.SO, "REUSEPORT")) {
                    try std.posix.setsockopt(
                        socket,
                        std.posix.SOL.SOCKET,
                        std.posix.SO.REUSEPORT,
                        &std.mem.toBytes(@as(c_int, 1)),
                    );
                } else {
                    try std.posix.setsockopt(
                        socket,
                        std.posix.SOL.SOCKET,
                        std.posix.SO.REUSEADDR,
                        &std.mem.toBytes(@as(c_int, 1)),
                    );
                }

                try socket_to_nonblocking(socket);
                try std.posix.bind(socket, &addr.any, addr.getOsSockLen());
                try std.posix.listen(socket, size);

                try tardy.entry(struct {
                    fn rt_start(rt: *Runtime, alloc: std.mem.Allocator, t_socket: *std.posix.socket_t) !void {
                        const pool: *Pool(Provision) = try alloc.create(Pool(Provision));
                        pool.* = try Pool(Provision).init(alloc, size, struct {
                            fn init(items: []Provision, all: anytype) void {
                                for (items) |*item| {
                                    item.buffer = all.alloc(u8, size) catch unreachable;
                                }
                            }
                        }.init, alloc);

                        try rt.storage.put("provision_pool", pool);
                        try rt.net.accept(.{
                            .socket = t_socket.*,
                            .func = accept_task,
                            .ctx = t_socket,
                        });
                    }
                }.rt_start, &socket);
            }
            </code>
        </pre>
    </section>
</div>
